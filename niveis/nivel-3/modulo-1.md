---
layout: page
title: "M√≥dulo 3.1: Algoritmos de IA Avan√ßados (PPO, SAC)"
permalink: /niveis/nivel-3/modulo-1/
---

# üß† M√≥dulo 3.1: Algoritmos de IA Avan√ßados (PPO, SAC)

**Domine os algoritmos state-of-the-art de Aprendizado por Refor√ßo!**

---

## üìã Informa√ß√µes do M√≥dulo

| Informa√ß√£o | Detalhes |
|------------|----------|
| **Dura√ß√£o estimada** | 15-20 horas |
| **N√≠vel** | Avan√ßado |
| **Pr√©-requisitos** | N√≠vel 2 conclu√≠do, conhecimento de RL b√°sico |
| **Ferramentas** | Python, Stable-Baselines3, PyTorch, TensorBoard |

---

## üéØ Objetivos de Aprendizado

Ao completar este m√≥dulo, voc√™ ser√° capaz de:

- [ ] Compreender profundamente PPO (Proximal Policy Optimization)
- [ ] Implementar e otimizar SAC (Soft Actor-Critic)
- [ ] Comparar e escolher algoritmos apropriados para diferentes tarefas
- [ ] Realizar tuning avan√ßado de hiperpar√¢metros
- [ ] Treinar pol√≠ticas com ambientes vetorizados
- [ ] Monitorar e debugar treinamento com TensorBoard
- [ ] Analisar converg√™ncia e performance

---

## üìö Conte√∫do Te√≥rico

### 1. Revis√£o: Fundamentos de Reinforcement Learning

Antes de mergulharmos nos algoritmos avan√ßados, vamos revisar conceitos essenciais:

#### Estado, A√ß√£o e Recompensa

```python
# Estrutura b√°sica de um ambiente de RL
class RobotEnvironment:
    def __init__(self):
        self.state = None

    def reset(self):
        """Retorna estado inicial"""
        return initial_state

    def step(self, action):
        """Executa a√ß√£o e retorna (pr√≥ximo_estado, recompensa, done, info)"""
        next_state = self.transition(self.state, action)
        reward = self.compute_reward(next_state)
        done = self.is_terminal(next_state)
        return next_state, reward, done, {}
```

#### Pol√≠tica vs. Value Function

- **Pol√≠tica (œÄ)**: Mapeia estados para a√ß√µes
  - Estoc√°stica: œÄ(a|s) = probabilidade de a√ß√£o a dado estado s
  - Determin√≠stica: a = œÄ(s)

- **Value Function (V)**: Valor esperado de um estado
  - V^œÄ(s) = E[‚àëŒ≥^t * r_t | s_0=s, œÄ]

- **Q-Function**: Valor esperado de a√ß√£o em estado
  - Q^œÄ(s,a) = E[‚àëŒ≥^t * r_t | s_0=s, a_0=a, œÄ]

#### Exploration vs. Exploitation

**Problema fundamental do RL**: balancear explora√ß√£o (tentar coisas novas) vs. explora√ß√£o (usar o que j√° funciona).

**Estrat√©gias comuns:**
- Œµ-greedy (usado em DQN)
- Entropy regularization (usado em SAC)
- Noise no espa√ßo de a√ß√µes (usado em DDPG)

#### Reward Shaping

Projetar boas fun√ß√µes de recompensa √© **cr√≠tico** para o sucesso do RL.

**Princ√≠pios:**
- **Densas**: Feedback frequente
- **Normalizadas**: Recompensas na mesma escala
- **Sem contradi√ß√µes**: N√£o incentivar comportamentos opostos
- **Alinhadas**: Refletir objetivo real

**Exemplo pr√°tico - Rob√¥ andando:**

```python
def compute_reward_walking(state):
    """Fun√ß√£o de recompensa bem projetada para caminhar"""

    # 1. Recompensa principal: velocidade para frente
    forward_velocity = state['base_velocity_x']
    reward = forward_velocity * 1.0  # Peso principal

    # 2. Penalidade por queda
    base_height = state['base_height']
    if base_height < 0.3:  # Muito baixo = queda
        reward -= 10.0

    # 3. Incentivo por manter corpo reto
    body_orientation = state['base_roll']  # √Çngulo de inclina√ß√£o
    reward -= abs(body_orientation) * 0.5

    # 4. Penalidade leve por uso de energia
    torques = state['joint_torques']
    energy_cost = np.sum(np.square(torques)) * 0.001
    reward -= energy_cost

    # 5. Bonus por estabilidade
    if state['steps_without_falling'] > 100:
        reward += 2.0

    return reward
```

**Armadilhas comuns:**
- Recompensa muito esparsa (rob√¥ nunca aprende)
- Recompensa densa demais (overfitting a comportamentos locais)
- Recompensas conflitantes (ex: maximizar velocidade E minimizar movimento)

---

### 2. PPO (Proximal Policy Optimization)

#### O que √© PPO?

**PPO** √© o algoritmo de RL mais popular para rob√≥tica, desenvolvido pela OpenAI em 2017. Ele resolve um problema fundamental: como atualizar a pol√≠tica de forma **eficiente** sem causar mudan√ßas **dr√°sticas** que desestabilizem o treinamento.

**Paper original:** [Proximal Policy Optimization Algorithms (Schulman et al., 2017)](https://arxiv.org/abs/1707.06347)

#### Intui√ß√£o: Por que PPO?

Imagine que voc√™ est√° treinando um rob√¥ para andar:
- **Itera√ß√£o 1**: Rob√¥ aprende a dar pequenos passos
- **Itera√ß√£o 2**: Voc√™ atualiza a pol√≠tica de forma agressiva
- **Resultado**: Rob√¥ esquece tudo e volta a cair

**Problema**: Policy Gradient tradicional pode fazer updates muito grandes, destruindo o que foi aprendido.

**Solu√ß√£o do PPO**: Limitar o tamanho do update para garantir melhorias incrementais e est√°veis.

#### Matem√°tica do PPO (Simplificada)

O PPO otimiza uma fun√ß√£o objetivo que **limita** quanto a nova pol√≠tica pode diferir da antiga:

```
L^CLIP(Œ∏) = E[min(r_t(Œ∏) * A_t, clip(r_t(Œ∏), 1-Œµ, 1+Œµ) * A_t)]

Onde:
- r_t(Œ∏) = œÄ_Œ∏(a|s) / œÄ_Œ∏_old(a|s)  [raz√£o entre pol√≠tica nova e antiga]
- A_t = vantagem (qu√£o melhor que m√©dia √© tomar a√ß√£o a)
- Œµ = limite de clipping (tipicamente 0.2)
```

**Interpreta√ß√£o:**
- Se `r_t` estiver entre `[1-Œµ, 1+Œµ]`, aceita o update
- Se `r_t` tentar ir muito al√©m, **clippa** (limita) o valor

Isso garante que a nova pol√≠tica n√£o se afasta muito da antiga.

#### Componentes do PPO

**1. Actor-Critic Architecture**

```python
import torch
import torch.nn as nn

class PPOActorCritic(nn.Module):
    def __init__(self, obs_dim, action_dim):
        super().__init__()

        # Feature extractor compartilhado
        self.shared = nn.Sequential(
            nn.Linear(obs_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU()
        )

        # Actor: gera distribui√ß√£o de a√ß√µes
        self.actor_mean = nn.Linear(256, action_dim)
        self.actor_logstd = nn.Parameter(torch.zeros(action_dim))

        # Critic: estima value function
        self.critic = nn.Linear(256, 1)

    def forward(self, obs):
        features = self.shared(obs)

        # Pol√≠tica (Gaussian para a√ß√µes cont√≠nuas)
        action_mean = self.actor_mean(features)
        action_std = torch.exp(self.actor_logstd)

        # Value function
        value = self.critic(features)

        return action_mean, action_std, value

    def get_action(self, obs, deterministic=False):
        """Amostra a√ß√£o da pol√≠tica"""
        action_mean, action_std, value = self.forward(obs)

        if deterministic:
            return action_mean, value

        # Amostra de distribui√ß√£o Gaussian
        dist = torch.distributions.Normal(action_mean, action_std)
        action = dist.sample()
        log_prob = dist.log_prob(action).sum(dim=-1)

        return action, log_prob, value
```

**2. Generalized Advantage Estimation (GAE)**

GAE √© usado para calcular vantagens de forma mais est√°vel:

```python
def compute_gae(rewards, values, dones, gamma=0.99, lambda_=0.95):
    """
    Calcula Generalized Advantage Estimation

    GAE balanceia bias vs. variance no c√°lculo de vantagens:
    - Œª=0: baixa variance, alto bias (TD(0))
    - Œª=1: alta variance, baixo bias (Monte Carlo)
    - Œª=0.95: sweet spot emp√≠rico
    """
    advantages = []
    gae = 0

    for t in reversed(range(len(rewards))):
        if t == len(rewards) - 1:
            next_value = 0
        else:
            next_value = values[t + 1]

        # TD error: Œ¥ = r + Œ≥V(s') - V(s)
        delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]

        # GAE: A = Œ¥ + Œ≥ŒªŒ¥' + (Œ≥Œª)¬≤Œ¥'' + ...
        gae = delta + gamma * lambda_ * (1 - dones[t]) * gae
        advantages.insert(0, gae)

    return torch.tensor(advantages)
```

**3. PPO Update Loop**

```python
def ppo_update(policy, optimizer, states, actions, old_log_probs,
               advantages, returns, clip_epsilon=0.2, epochs=10):
    """
    Atualiza pol√≠tica usando PPO
    """
    for epoch in range(epochs):
        # Forward pass
        action_mean, action_std, values = policy(states)
        dist = torch.distributions.Normal(action_mean, action_std)
        new_log_probs = dist.log_prob(actions).sum(dim=-1)

        # Ratio r_t = œÄ_new / œÄ_old
        ratio = torch.exp(new_log_probs - old_log_probs)

        # PPO clipped objective
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1-clip_epsilon, 1+clip_epsilon) * advantages
        actor_loss = -torch.min(surr1, surr2).mean()

        # Value function loss
        value_loss = F.mse_loss(values.squeeze(), returns)

        # Entropy bonus (incentiva explora√ß√£o)
        entropy = dist.entropy().mean()

        # Loss total
        loss = actor_loss + 0.5 * value_loss - 0.01 * entropy

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(policy.parameters(), max_norm=0.5)
        optimizer.step()
```

#### Implementa√ß√£o Completa com Stable-Baselines3

```python
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
from stable_baselines3.common.callbacks import EvalCallback
import gymnasium as gym

# 1. Criar ambiente
def make_env():
    def _init():
        env = gym.make('HumanoidStandup-v4')
        return env
    return _init

# 2. Ambientes vetorizados (paralelo)
n_envs = 8
env = SubprocVecEnv([make_env() for _ in range(n_envs)])

# 3. Configurar PPO
model = PPO(
    policy="MlpPolicy",
    env=env,

    # Hiperpar√¢metros core
    learning_rate=3e-4,
    n_steps=2048,        # Steps por rollout por env
    batch_size=64,       # Minibatch size
    n_epochs=10,         # Epochs por update
    gamma=0.99,          # Discount factor
    gae_lambda=0.95,     # GAE lambda

    # PPO espec√≠fico
    clip_range=0.2,      # Epsilon de clipping
    clip_range_vf=None,  # Clipping no value function

    # Regulariza√ß√£o
    ent_coef=0.01,       # Entropy bonus
    vf_coef=0.5,         # Value function coef
    max_grad_norm=0.5,   # Gradient clipping

    # Network
    policy_kwargs=dict(
        net_arch=[dict(pi=[256, 256], vf=[256, 256])]
    ),

    # Logging
    verbose=1,
    tensorboard_log="./ppo_humanoid_tensorboard/"
)

# 4. Callback para avalia√ß√£o
eval_callback = EvalCallback(
    eval_env=gym.make('HumanoidStandup-v4'),
    best_model_save_path='./best_model/',
    log_path='./eval_logs/',
    eval_freq=10000,
    deterministic=True,
    render=False
)

# 5. Treinar
model.learn(
    total_timesteps=10_000_000,  # 10M steps
    callback=eval_callback
)

# 6. Salvar
model.save("ppo_humanoid_final")

# 7. Testar
obs, info = env.reset()
for _ in range(1000):
    action, _states = model.predict(obs, deterministic=True)
    obs, rewards, dones, truncated, infos = env.step(action)
    env.render()
```

#### Quando Usar PPO?

**PPO √© ideal para:**
- ‚úÖ Tarefas de locomo√ß√£o (andar, correr, pular)
- ‚úÖ Manipula√ß√£o rob√≥tica
- ‚úÖ Controle cont√≠nuo multi-dimensional
- ‚úÖ Quando voc√™ precisa de estabilidade no treinamento
- ‚úÖ Quando voc√™ tem muitos cores (paraleliza bem)

**PPO n√£o √© ideal para:**
- ‚ùå Tarefas que exigem explora√ß√£o agressiva
- ‚ùå Ambientes muito esparsos
- ‚ùå Quando voc√™ precisa de sample efficiency extrema

---

### 3. SAC (Soft Actor-Critic)

#### O que √© SAC?

**SAC** (Soft Actor-Critic) √© um algoritmo de RL off-policy que maximiza tanto a recompensa quanto a **entropia** da pol√≠tica. Foi desenvolvido pela UC Berkeley em 2018.

**Paper original:** [Soft Actor-Critic: Off-Policy Maximum Entropy Deep RL (Haarnoja et al., 2018)](https://arxiv.org/abs/1801.01290)

#### Intui√ß√£o: Maximum Entropy RL

SAC resolve um problema diferente do PPO:

**Objetivo tradicional de RL:**
```
Maximize: E[‚àë Œ≥^t * r_t]
```

**Objetivo do SAC (Maximum Entropy):**
```
Maximize: E[‚àë Œ≥^t * (r_t + Œ± * H(œÄ(¬∑|s_t)))]

Onde H(œÄ) = entropia da pol√≠tica
```

**Por que adicionar entropia?**
1. **Explora√ß√£o autom√°tica**: Pol√≠tica naturalmente explora
2. **Robustez**: Aprende m√∫ltiplas solu√ß√µes
3. **Transfer**: Generaliza melhor para novos ambientes

**Analogia**: Ao inv√©s de aprender "uma √∫nica forma certa" de pegar um objeto, o rob√¥ aprende "v√°rias formas que funcionam", tornando-o mais adapt√°vel.

#### Arquitetura do SAC

SAC usa **tr√™s redes neurais**:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SACActorCritic(nn.Module):
    def __init__(self, obs_dim, action_dim):
        super().__init__()

        # Actor: Pol√≠tica estoc√°stica (Gaussian Squashed)
        self.actor = nn.Sequential(
            nn.Linear(obs_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU()
        )
        self.mean = nn.Linear(256, action_dim)
        self.log_std = nn.Linear(256, action_dim)

        # Critic 1: Q-function
        self.q1 = nn.Sequential(
            nn.Linear(obs_dim + action_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )

        # Critic 2: Q-function (double Q-learning)
        self.q2 = nn.Sequential(
            nn.Linear(obs_dim + action_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )

    def get_action(self, obs, deterministic=False):
        """Amostra a√ß√£o usando reparameterization trick"""
        features = self.actor(obs)
        mean = self.mean(features)
        log_std = self.log_std(features)
        log_std = torch.clamp(log_std, -20, 2)  # Estabilidade
        std = torch.exp(log_std)

        if deterministic:
            action = mean
        else:
            # Reparameterization trick
            normal = torch.distributions.Normal(mean, std)
            z = normal.rsample()  # Permite backprop
            action = torch.tanh(z)  # Squash para [-1, 1]

        # Log probability (com corre√ß√£o para tanh)
        log_prob = normal.log_prob(z) - torch.log(1 - action.pow(2) + 1e-6)
        log_prob = log_prob.sum(dim=-1, keepdim=True)

        return action, log_prob

    def get_q_values(self, obs, action):
        """Retorna Q-values das duas redes"""
        x = torch.cat([obs, action], dim=-1)
        q1 = self.q1(x)
        q2 = self.q2(x)
        return q1, q2
```

#### Componentes Chave do SAC

**1. Replay Buffer (Experience Replay)**

SAC √© **off-policy**: aprende de transi√ß√µes antigas armazenadas:

```python
import numpy as np

class ReplayBuffer:
    def __init__(self, obs_dim, action_dim, capacity=1_000_000):
        self.capacity = capacity
        self.ptr = 0
        self.size = 0

        self.states = np.zeros((capacity, obs_dim), dtype=np.float32)
        self.actions = np.zeros((capacity, action_dim), dtype=np.float32)
        self.rewards = np.zeros((capacity, 1), dtype=np.float32)
        self.next_states = np.zeros((capacity, obs_dim), dtype=np.float32)
        self.dones = np.zeros((capacity, 1), dtype=np.float32)

    def add(self, state, action, reward, next_state, done):
        self.states[self.ptr] = state
        self.actions[self.ptr] = action
        self.rewards[self.ptr] = reward
        self.next_states[self.ptr] = next_state
        self.dones[self.ptr] = done

        self.ptr = (self.ptr + 1) % self.capacity
        self.size = min(self.size + 1, self.capacity)

    def sample(self, batch_size):
        idx = np.random.randint(0, self.size, size=batch_size)
        return (
            torch.FloatTensor(self.states[idx]),
            torch.FloatTensor(self.actions[idx]),
            torch.FloatTensor(self.rewards[idx]),
            torch.FloatTensor(self.next_states[idx]),
            torch.FloatTensor(self.dones[idx])
        )
```

**2. Target Networks**

SAC usa redes target para estabilizar treinamento:

```python
import copy

# Criar redes target
target_critic = copy.deepcopy(critic)

# Soft update (Polyak averaging)
def soft_update(target, source, tau=0.005):
    """
    Target: Œ∏' ‚Üê œÑŒ∏ + (1-œÑ)Œ∏'
    """
    for target_param, param in zip(target.parameters(), source.parameters()):
        target_param.data.copy_(
            tau * param.data + (1.0 - tau) * target_param.data
        )
```

**3. Automatic Entropy Tuning**

SAC ajusta automaticamente o coeficiente de entropia Œ±:

```python
# Entropia alvo (heur√≠stica: -dim(A))
target_entropy = -action_dim

# Œ± como vari√°vel trein√°vel (em log-space para estabilidade)
log_alpha = torch.zeros(1, requires_grad=True)
alpha_optimizer = torch.optim.Adam([log_alpha], lr=3e-4)

# Atualizar Œ±
def update_alpha(log_prob):
    alpha = log_alpha.exp()
    alpha_loss = -(log_alpha * (log_prob + target_entropy).detach()).mean()

    alpha_optimizer.zero_grad()
    alpha_loss.backward()
    alpha_optimizer.step()

    return alpha.detach()
```

**4. SAC Update Loop**

```python
def sac_update(actor, critic, target_critic, replay_buffer,
               alpha, batch_size=256):
    # Amostra batch
    states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)

    # --- Atualizar Critic ---
    with torch.no_grad():
        # Amostra pr√≥xima a√ß√£o da pol√≠tica atual
        next_actions, next_log_probs = actor.get_action(next_states)

        # Q-target usando min de duas redes (Clipped Double Q-learning)
        q1_next, q2_next = target_critic.get_q_values(next_states, next_actions)
        q_next = torch.min(q1_next, q2_next) - alpha * next_log_probs
        q_target = rewards + gamma * (1 - dones) * q_next

    # Q atual
    q1, q2 = critic.get_q_values(states, actions)

    # Loss de critic (MSE)
    critic_loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)

    critic_optimizer.zero_grad()
    critic_loss.backward()
    critic_optimizer.step()

    # --- Atualizar Actor ---
    new_actions, log_probs = actor.get_action(states)
    q1_new, q2_new = critic.get_q_values(states, new_actions)
    q_new = torch.min(q1_new, q2_new)

    # Maximizar Q - entropia (equivalent: minimizar -Q + Œ±*log_prob)
    actor_loss = (alpha * log_probs - q_new).mean()

    actor_optimizer.zero_grad()
    actor_loss.backward()
    actor_optimizer.step()

    # --- Atualizar Alpha ---
    alpha = update_alpha(log_probs.detach())

    # --- Soft update de target networks ---
    soft_update(target_critic, critic, tau=0.005)

    return actor_loss.item(), critic_loss.item()
```

#### Implementa√ß√£o Completa com Stable-Baselines3

```python
from stable_baselines3 import SAC
from stable_baselines3.common.noise import NormalActionNoise
import gymnasium as gym
import numpy as np

# 1. Criar ambiente
env = gym.make('Humanoid-v4')

# 2. Configurar SAC
model = SAC(
    policy="MlpPolicy",
    env=env,

    # Hiperpar√¢metros core
    learning_rate=3e-4,
    buffer_size=1_000_000,   # Replay buffer size
    learning_starts=10000,   # Steps antes de come√ßar treino
    batch_size=256,          # Batch size
    tau=0.005,               # Soft update coef
    gamma=0.99,              # Discount factor

    # SAC espec√≠fico
    train_freq=1,            # Atualiza a cada step
    gradient_steps=1,        # Gradient steps por update
    ent_coef='auto',         # Auto-tune entropy coef
    target_entropy='auto',   # -dim(A)

    # Network architecture
    policy_kwargs=dict(
        net_arch=[256, 256],
        log_std_init=-3
    ),

    # Logging
    verbose=1,
    tensorboard_log="./sac_humanoid_tensorboard/"
)

# 3. Treinar
model.learn(
    total_timesteps=3_000_000,
    log_interval=10
)

# 4. Salvar e testar
model.save("sac_humanoid_final")

obs, info = env.reset()
for _ in range(1000):
    action, _states = model.predict(obs, deterministic=True)
    obs, reward, done, truncated, info = env.step(action)
    env.render()
```

#### Quando Usar SAC?

**SAC √© ideal para:**
- ‚úÖ Tarefas que exigem **explora√ß√£o intensa**
- ‚úÖ Manipula√ß√£o de objetos complexa
- ‚úÖ Ambientes cont√≠nuos com recompensas esparsas
- ‚úÖ Quando voc√™ precisa de **sample efficiency** (aprende r√°pido)
- ‚úÖ Quando voc√™ quer robustez e generaliza√ß√£o

**SAC n√£o √© ideal para:**
- ‚ùå A√ß√µes discretas (use DQN ou PPO)
- ‚ùå Quando voc√™ tem muitos cores ociosos (n√£o paraleliza como PPO)
- ‚ùå Ambientes muito simples (overhead de replay buffer)

---

### 4. PPO vs SAC: Compara√ß√£o Detalhada

| Aspecto | PPO | SAC |
|---------|-----|-----|
| **Tipo** | On-policy | Off-policy |
| **Paraleliza√ß√£o** | Excelente (8-32 envs) | Limitada (1 env) |
| **Sample Efficiency** | Baixa-M√©dia | Alta |
| **Estabilidade** | Muito alta | Alta |
| **Explora√ß√£o** | Œµ-decay ou noise | Automatic (entropy) |
| **Mem√≥ria** | Baixa | Alta (replay buffer) |
| **Velocidade por step** | R√°pida | Mais lenta |
| **Hiperpar√¢metros** | Sens√≠vel | Mais robusto |
| **Converg√™ncia** | Gradual | R√°pida (se convergir) |

**Regra de ouro:**
- **Locomo√ß√£o/Controle simples**: PPO
- **Manipula√ß√£o/Explora√ß√£o**: SAC
- **Muitos cores dispon√≠veis**: PPO
- **Sample efficiency cr√≠tica**: SAC
- **N√£o sabe qual usar**: Comece com PPO

---

### 5. Tuning de Hiperpar√¢metros

#### Hiperpar√¢metros Cr√≠ticos do PPO

**Learning Rate (lr)**
- Range: `1e-5` a `1e-3`
- Padr√£o: `3e-4`
- **Como ajustar**: Se treinamento oscilar muito, diminua. Se muito lento, aumente.

```python
# Decaying learning rate
from torch.optim.lr_scheduler import LinearLR

scheduler = LinearLR(
    optimizer,
    start_factor=1.0,
    end_factor=0.1,
    total_iters=1000
)
```

**n_steps (Rollout Length)**
- Range: `512` a `4096`
- Padr√£o: `2048`
- **Trade-off**: Maior = mais est√°vel, mas menos updates

**Clip Range (Œµ)**
- Range: `0.1` a `0.3`
- Padr√£o: `0.2`
- **Como ajustar**: Se updates muito conservadores, aumente. Se inst√°vel, diminua.

**GAE Lambda (Œª)**
- Range: `0.9` a `0.99`
- Padr√£o: `0.95`
- **Trade-off**: Menor = menos variance, mais bias. Maior = mais variance, menos bias.

#### Hiperpar√¢metros Cr√≠ticos do SAC

**Learning Rate**
- Range: `1e-5` a `1e-3`
- Padr√£o: `3e-4`
- **Dica**: SAC geralmente tolera LR maiores que PPO

**Batch Size**
- Range: `64` a `512`
- Padr√£o: `256`
- **Trade-off**: Maior = mais est√°vel, mas mais lento

**Buffer Size**
- Range: `100k` a `10M`
- Padr√£o: `1M`
- **Limite**: RAM dispon√≠vel (1M steps ‚âà 2-4 GB)

**Tau (Soft Update)**
- Range: `0.001` a `0.01`
- Padr√£o: `0.005`
- **Como ajustar**: Menor = mais est√°vel, mas mais lento

#### Grid Search Automatizado

```python
from stable_baselines3 import PPO
import optuna

def objective(trial):
    # Hiperpar√¢metros a otimizar
    lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)
    n_steps = trial.suggest_categorical('n_steps', [512, 1024, 2048, 4096])
    clip_range = trial.suggest_uniform('clip_range', 0.1, 0.3)
    gae_lambda = trial.suggest_uniform('gae_lambda', 0.9, 0.99)

    # Criar modelo
    model = PPO(
        'MlpPolicy',
        env='Humanoid-v4',
        learning_rate=lr,
        n_steps=n_steps,
        clip_range=clip_range,
        gae_lambda=gae_lambda,
        verbose=0
    )

    # Treinar por um per√≠odo curto
    model.learn(total_timesteps=100_000)

    # Avaliar performance
    mean_reward = evaluate_policy(model, env, n_eval_episodes=10)

    return mean_reward

# Otimizar com Optuna
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)

print(f"Melhores hiperpar√¢metros: {study.best_params}")
```

---

## üíª Pr√°tica Hands-On

### Projeto 1: Treinar Humanoid com PPO

**Objetivo:** Treinar um rob√¥ humanoide para ficar em p√© usando PPO.

```python
# train_humanoid_ppo.py
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv
from stable_baselines3.common.callbacks import CheckpointCallback
import gymnasium as gym

# Criar 8 ambientes paralelos
def make_env():
    def _init():
        return gym.make('HumanoidStandup-v4')
    return _init

env = SubprocVecEnv([make_env() for _ in range(8)])

# Callback para salvar checkpoints
checkpoint_callback = CheckpointCallback(
    save_freq=100_000,
    save_path='./checkpoints/',
    name_prefix='humanoid_ppo'
)

# Configurar PPO otimizado
model = PPO(
    'MlpPolicy',
    env,
    learning_rate=3e-4,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.2,
    ent_coef=0.01,
    verbose=1,
    tensorboard_log='./ppo_humanoid/'
)

# Treinar por 10M steps (~3-4 horas em CPU moderno)
model.learn(
    total_timesteps=10_000_000,
    callback=checkpoint_callback
)

model.save('humanoid_ppo_final')
```

**Executar:**
```bash
python train_humanoid_ppo.py

# Monitorar com TensorBoard
tensorboard --logdir ./ppo_humanoid/
```

**M√©tricas esperadas:**
- Ep Reward Mean: 0 ‚Üí 5000+ em 5-10M steps
- Ep Length Mean: ~500 (m√°ximo do ambiente)
- Value Loss: Deve estabilizar em ~100-200

---

### Projeto 2: Comparar PPO vs SAC

**Objetivo:** Treinar o mesmo ambiente com ambos e comparar.

```python
# compare_algorithms.py
import gymnasium as gym
from stable_baselines3 import PPO, SAC
import matplotlib.pyplot as plt
import pandas as pd

env_name = 'Ant-v4'  # Formiga quadr√∫pede

# 1. Treinar PPO
print("Treinando PPO...")
ppo_model = PPO('MlpPolicy', env_name, verbose=0)
ppo_rewards = []

for i in range(100):
    ppo_model.learn(total_timesteps=10_000, reset_num_timesteps=False)
    mean_reward = evaluate_policy(ppo_model, env_name, n_eval_episodes=5)
    ppo_rewards.append(mean_reward)
    print(f"PPO Step {i*10000}: {mean_reward}")

# 2. Treinar SAC
print("Treinando SAC...")
sac_model = SAC('MlpPolicy', env_name, verbose=0)
sac_rewards = []

for i in range(100):
    sac_model.learn(total_timesteps=10_000, reset_num_timesteps=False)
    mean_reward = evaluate_policy(sac_model, env_name, n_eval_episodes=5)
    sac_rewards.append(mean_reward)
    print(f"SAC Step {i*10000}: {mean_reward}")

# 3. Plotar compara√ß√£o
plt.figure(figsize=(10, 6))
plt.plot(ppo_rewards, label='PPO', linewidth=2)
plt.plot(sac_rewards, label='SAC', linewidth=2)
plt.xlabel('Training Steps (x10k)')
plt.ylabel('Mean Episode Reward')
plt.title(f'PPO vs SAC on {env_name}')
plt.legend()
plt.grid(True)
plt.savefig('ppo_vs_sac_comparison.png')
plt.show()

# 4. Estat√≠sticas finais
print("\n=== Resultados Finais ===")
print(f"PPO Final Reward: {ppo_rewards[-1]:.2f}")
print(f"SAC Final Reward: {sac_rewards[-1]:.2f}")
print(f"PPO converge em ~{len([r for r in ppo_rewards if r > 0.8*max(ppo_rewards)])}k steps")
print(f"SAC converge em ~{len([r for r in sac_rewards if r > 0.8*max(sac_rewards)])}k steps")
```

---

### Projeto 3: Custom Reward Function

**Objetivo:** Criar fun√ß√£o de recompensa customizada para tarefa espec√≠fica.

**Tarefa:** Rob√¥ deve pegar um cubo e coloc√°-lo em uma caixa.

```python
import gymnasium as gym
from gymnasium import spaces
import numpy as np

class PickAndPlaceEnv(gym.Env):
    """Ambiente customizado: pegar e colocar objeto"""

    def __init__(self):
        super().__init__()

        # Estado: [posi√ß√£o_rob√¥(3), vel_rob√¥(3), posi√ß√£o_cubo(3), posi√ß√£o_alvo(3)]
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(12,), dtype=np.float32
        )

        # A√ß√£o: [delta_x, delta_y, delta_z, gripper_force]
        self.action_space = spaces.Box(
            low=-1, high=1, shape=(4,), dtype=np.float32
        )

        self.reset()

    def reset(self, seed=None):
        super().reset(seed=seed)

        # Posi√ß√£o inicial aleat√≥ria
        self.robot_pos = np.array([0.0, 0.0, 0.5])
        self.robot_vel = np.zeros(3)
        self.cube_pos = np.random.uniform([-0.3, -0.3, 0.0], [0.3, 0.3, 0.0])
        self.target_pos = np.array([0.5, 0.5, 0.2])

        self.cube_grasped = False
        self.steps = 0

        return self._get_obs(), {}

    def _get_obs(self):
        return np.concatenate([
            self.robot_pos,
            self.robot_vel,
            self.cube_pos,
            self.target_pos
        ])

    def step(self, action):
        # Mover rob√¥
        self.robot_pos += action[:3] * 0.05
        self.robot_vel = action[:3]

        # Verificar se pegou cubo
        dist_to_cube = np.linalg.norm(self.robot_pos - self.cube_pos)
        if dist_to_cube < 0.05 and action[3] > 0.5:  # Gripper fechado
            self.cube_grasped = True

        # Se pegou, cubo move com rob√¥
        if self.cube_grasped:
            self.cube_pos = self.robot_pos.copy()

        # Calcular recompensa (aqui est√° a m√°gica!)
        reward = self._compute_reward()

        # Termina se alcan√ßou objetivo ou timeout
        dist_to_target = np.linalg.norm(self.cube_pos - self.target_pos)
        done = (dist_to_target < 0.1 and self.cube_grasped) or self.steps > 500

        self.steps += 1

        return self._get_obs(), reward, done, False, {}

    def _compute_reward(self):
        """
        Design de recompensa multi-stage para pick-and-place
        """
        reward = 0.0

        dist_robot_to_cube = np.linalg.norm(self.robot_pos - self.cube_pos)
        dist_cube_to_target = np.linalg.norm(self.cube_pos - self.target_pos)

        # FASE 1: Alcan√ßar cubo
        if not self.cube_grasped:
            # Recompensa densa por aproximar do cubo
            reward += 1.0 / (1.0 + dist_robot_to_cube)

            # Bonus por tocar no cubo
            if dist_robot_to_cube < 0.05:
                reward += 2.0

        # FASE 2: Pegar cubo
        if self.cube_grasped:
            reward += 5.0  # Bonus por pegar (one-time)

            # Recompensa densa por levar ao alvo
            reward += 2.0 / (1.0 + dist_cube_to_target)

            # Bonus enorme por colocar no lugar
            if dist_cube_to_target < 0.1:
                reward += 20.0

        # PENALIDADES
        # - Penalidade leve por movimentos bruscos (energia)
        velocity_penalty = np.sum(np.square(self.robot_vel)) * 0.01
        reward -= velocity_penalty

        # - Penalidade por sair dos limites
        if np.any(np.abs(self.robot_pos[:2]) > 1.0):
            reward -= 1.0

        return reward

# Treinar com PPO
env = PickAndPlaceEnv()
model = PPO('MlpPolicy', env, verbose=1)
model.learn(total_timesteps=500_000)
model.save('pick_and_place_ppo')
```

---

## üìä Monitoramento com TensorBoard

TensorBoard √© essencial para debugar treinamento de RL:

```bash
# Instalar
pip install tensorboard

# Executar
tensorboard --logdir ./logs/
```

**M√©tricas importantes:**

1. **Episode Reward Mean**: Recompensa m√©dia por epis√≥dio
   - Deve aumentar consistentemente
   - Se oscila muito: reward shaping ruim ou lr muito alta

2. **Episode Length Mean**: Dura√ß√£o dos epis√≥dios
   - Se sempre max_steps: rob√¥ n√£o est√° terminando tarefa
   - Se muito curto: rob√¥ est√° falhando r√°pido

3. **Policy Loss**: Loss do actor
   - Deve diminuir e estabilizar
   - Se aumenta: treinamento inst√°vel

4. **Value Loss**: Loss do critic
   - Deve diminuir
   - Se muito alta: critic n√£o consegue estimar valores

5. **Explained Variance**: Qu√£o bem critic prev√™ retornos
   - Ideal: > 0.7
   - Baixa: critic ruim, vai prejudicar actor

6. **Entropy**: Aleatoriedade da pol√≠tica
   - Deve diminuir gradualmente
   - Se cai r√°pido demais: explora√ß√£o insuficiente

**Exemplo de an√°lise:**

```python
# Carregar logs do TensorBoard
from tensorboard.backend.event_processing import event_accumulator
import matplotlib.pyplot as plt

ea = event_accumulator.EventAccumulator('logs/PPO_1/')
ea.Reload()

# Extrair m√©tricas
rewards = [(s.step, s.value) for s in ea.Scalars('rollout/ep_rew_mean')]
steps, values = zip(*rewards)

# Plotar
plt.figure(figsize=(12, 6))
plt.plot(steps, values)
plt.xlabel('Steps')
plt.ylabel('Mean Episode Reward')
plt.title('Learning Curve')
plt.grid(True)
plt.show()
```

---

## üèÜ Projeto Final

### Sistema de Locomo√ß√£o B√≠pede Avan√ßado

**Objetivo:** Treinar um rob√¥ humanoide para:
1. Andar para frente
2. Andar para tr√°s
3. Virar esquerda/direita
4. Manter equil√≠brio em terrenos irregulares

**Especifica√ß√µes:**
- Ambiente: `Humanoid-v4` ou Isaac Sim
- Algoritmo: PPO ou SAC (sua escolha)
- Crit√©rio de sucesso: Andar 10m sem cair

```python
# advanced_humanoid_locomotion.py
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv, VecNormalize
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback

def make_env():
    def _init():
        env = gym.make('Humanoid-v4')
        return env
    return _init

# 1. Criar ambientes paralelos
n_envs = 16
env = SubprocVecEnv([make_env() for _ in range(n_envs)])

# 2. Normaliza√ß√£o de observa√ß√µes e recompensas
env = VecNormalize(
    env,
    norm_obs=True,
    norm_reward=True,
    clip_obs=10.0,
    clip_reward=10.0
)

# 3. Callbacks
checkpoint_callback = CheckpointCallback(
    save_freq=50_000,
    save_path='./checkpoints/',
    name_prefix='humanoid_advanced'
)

eval_callback = EvalCallback(
    eval_env=gym.make('Humanoid-v4'),
    best_model_save_path='./best_model/',
    log_path='./eval_logs/',
    eval_freq=20_000,
    deterministic=True
)

# 4. Configurar modelo com hiperpar√¢metros otimizados
model = PPO(
    'MlpPolicy',
    env,
    learning_rate=3e-4,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.2,
    ent_coef=0.005,  # Menos entropia para comportamento mais consistente
    vf_coef=0.5,
    max_grad_norm=0.5,
    policy_kwargs=dict(
        net_arch=[dict(pi=[256, 256], vf=[256, 256])],
        activation_fn=torch.nn.ReLU
    ),
    verbose=1,
    tensorboard_log='./logs/'
)

# 5. Treinar
print("Iniciando treinamento...")
model.learn(
    total_timesteps=20_000_000,  # 20M steps
    callback=[checkpoint_callback, eval_callback]
)

# 6. Salvar modelo final
model.save('humanoid_advanced_final')
env.save('vec_normalize.pkl')

# 7. Testar
print("Testando modelo treinado...")
obs = env.reset()
for _ in range(5000):
    action, _states = model.predict(obs, deterministic=True)
    obs, rewards, dones, info = env.step(action)
    env.render()

print("Treinamento completo!")
```

**Crit√©rios de avalia√ß√£o:**
- [ ] Rob√¥ anda 10m sem cair
- [ ] Velocidade m√©dia > 0.5 m/s
- [ ] Mant√©m equil√≠brio (corpo reto)
- [ ] C√≥digo documentado e limpo
- [ ] Logs e gr√°ficos de treinamento
- [ ] An√°lise de performance

---

## üìö Recursos Adicionais

### Papers Fundamentais

1. **PPO**: [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347) - Schulman et al., 2017
2. **SAC**: [Soft Actor-Critic: Off-Policy Maximum Entropy Deep RL](https://arxiv.org/abs/1801.01290) - Haarnoja et al., 2018
3. **GAE**: [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438) - Schulman et al., 2015

### Tutoriais e Cursos

- [Stable-Baselines3 Documentation](https://stable-baselines3.readthedocs.io/)
- [Spinning Up in Deep RL (OpenAI)](https://spinningup.openai.com/)
- [Deep RL Course (Hugging Face)](https://huggingface.co/deep-rl-course)

### Bibliotecas

```bash
pip install stable-baselines3[extra]
pip install gymnasium[mujoco]
pip install tensorboard
pip install optuna  # Hyperparameter tuning
```

### Comunidades

- [r/reinforcementlearning](https://www.reddit.com/r/reinforcementlearning/)
- [RL Discord](https://discord.gg/xhfNqQv)
- [Stable-Baselines3 Discussions](https://github.com/DLR-RM/stable-baselines3/discussions)

---

## ‚úÖ Checklist de Conclus√£o

- [ ] Entendo a diferen√ßa entre on-policy e off-policy
- [ ] Implementei PPO do zero (ou com SB3)
- [ ] Implementei SAC do zero (ou com SB3)
- [ ] Comparei PPO vs SAC em um ambiente
- [ ] Projetei uma reward function customizada
- [ ] Fiz tuning de hiperpar√¢metros
- [ ] Treinei com ambientes vetorizados
- [ ] Analisei logs no TensorBoard
- [ ] Completei o projeto final
- [ ] Publiquei c√≥digo no GitHub

---

## üöÄ Pr√≥ximos Passos

Parab√©ns por dominar algoritmos avan√ßados de RL! Agora voc√™ est√° pronto para:

1. **M√≥dulo 3.2**: Vis√£o Computacional para Rob√≥tica
2. **M√≥dulo 3.3**: Large Behavior Models (LBMs)
3. **N√≠vel 4**: Aplica√ß√µes Profissionais

---

**√öltima atualiza√ß√£o:** 2025-10-29
**Autor:** Programa FTH
**Licen√ßa:** MIT
